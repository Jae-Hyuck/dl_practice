{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN_long_char\n",
    "다음번에 나올 char 맞추기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes 25\n",
      "0 if you wan -> f you want\n",
      "20  a ship, d -> a ship, do\n",
      "40 up people  -> p people t\n",
      "60 o collect  ->  collect w\n",
      "80 on't assig -> n't assign\n",
      "100 ks and wor -> s and work\n",
      "120 her teach  -> er teach t\n",
      "140 ng for the -> g for the \n",
      "160 mmensity o -> mensity of\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")\n",
    "\n",
    "char_set = list(set(sentence))\n",
    "char_dic = {w: i for i, w in enumerate(char_set)}\n",
    "\n",
    "# data_dim = len(char_set)\n",
    "hidden_size = 20\n",
    "num_classes = len(char_set)\n",
    "sequence_length = 10  # Any arbitrary number\n",
    "learning_rate = 0.1\n",
    "print('num_classes', num_classes)\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:i+sequence_length]\n",
    "    y_str = sentence[i+1:i+sequence_length+1]\n",
    "    if i % 20 == 0:\n",
    "        print(i, x_str, '->', y_str)\n",
    "\n",
    "    x = [char_dic[c] for c in x_str]  # x str to index\n",
    "    y = [char_dic[c] for c in y_str]  # y str to index\n",
    "\n",
    "    dataX.append(x)\n",
    "    dataY.append(y)\n",
    "\n",
    "batch_size = len(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"one_hot:0\", shape=(?, 10, 25), dtype=float32)\n",
      "0 skhggggbbb 3.2196846\n",
      "100 l you want 0.24567482\n",
      "200 f you want 0.23203954\n",
      "300 l you want 0.23047268\n",
      "400 t you want 0.229613\n",
      "500 l you want 0.22924514\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea."
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "\n",
    "# One-hot encoding\n",
    "X_one_hot = tf.one_hot(X, num_classes)\n",
    "print(X_one_hot)  # check out the shape\n",
    "\n",
    "# Make a lstm cell with hidden_size (each unit output vector size)\n",
    "def lstm_cell():\n",
    "    cell = rnn.BasicLSTMCell(hidden_size, state_is_tuple=True)\n",
    "    return cell\n",
    "\n",
    "# LSTM을 쌓기 위해 사용.\n",
    "# LSTM layer 하나만 사용할꺼면 그냥 BasicLSTMCell을 dynamic_rnn으로 바로 넘기면 됨.\n",
    "multi_cells = rnn.MultiRNNCell([lstm_cell() for _ in range(2)], state_is_tuple=True)\n",
    "\n",
    "# outputs: unfolding size x hidden size, state = hidden size\n",
    "outputs, _states = tf.nn.dynamic_rnn(multi_cells, X_one_hot, dtype=tf.float32)\n",
    "\n",
    "# FC layer\n",
    "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
    "outputs = tf.contrib.layers.fully_connected(X_for_fc, num_classes, activation_fn=None)\n",
    "\n",
    "# reshape out for sequence_loss\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "# 여기에 softmax도 넣어주는게 더 낫지 않을까?\n",
    "\n",
    "# All weights are 1 (equal weights)\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "\n",
    "# sequence output의 loss를 계산해주는 편한 방법.\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=outputs, targets=Y, weights=weights)\n",
    "mean_loss = tf.reduce_mean(sequence_loss)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(mean_loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(501):\n",
    "    _, l, results = sess.run([train_op, mean_loss, outputs], feed_dict={X: dataX, Y: dataY})\n",
    "    if i % 100 == 0:\n",
    "        index = np.argmax(results[0], axis=1)\n",
    "        print(i, ''.join([char_set[t] for t in index]), l)\n",
    "\n",
    "# Let's print the last char of each result to check it works\n",
    "results = sess.run(outputs, feed_dict={X: dataX})\n",
    "for j, result in enumerate(results):\n",
    "    index = np.argmax(result, axis=1)\n",
    "    if j is 0:  # print all for the first result to make a sentence\n",
    "        print(''.join([char_set[t] for t in index]), end='')\n",
    "    else:\n",
    "        print(char_set[index[-1]], end='')\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
